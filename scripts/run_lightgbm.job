#!/bin/bash
#SBATCH --partition=cpu
#SBATCH --job-name=lgbm_test
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=32G
#SBATCH --time=00:30:00
#SBATCH --output=output/tabular/slurm_output_%A.out
#SBATCH --error=output/tabular/slurm_error_%A.err

export OUTPUT_DIR=output/tabular/lgbm_${SLURM_JOB_ID}
mkdir -p $OUTPUT_DIR

# Load modules
module purge
module load 2023
module load Python/3.11.3-GCCcore-12.3.0
module load Python-bundle-PyPI/2023.06-GCCcore-12.3.0
module load matplotlib/3.7.2-gfbf-2023a
module load scikit-learn/1.3.1-gfbf-2023a

# Install missing packages if needed
python -c "import lightgbm" 2>/dev/null || pip install --user lightgbm
python -c "import optuna" 2>/dev/null || pip install --user optuna
python -c "import joblib" 2>/dev/null || pip install --user joblib
python -c "import pandas" 2>/dev/null || pip install --user pandas

# Set LightGBM threading (use available CPUs)
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OPENBLAS_NUM_THREADS=$SLURM_CPUS_PER_TASK

# Move into your project directory
cd $HOME/eSTGNN

# Create directories on scratch space
mkdir -p $TMPDIR/data/processed

# Copy data to scratch (fast local disk)
echo "Copying data to scratch space..."
cp $HOME/eSTGNN/data/processed/tabular_dataset.joblib \
   $TMPDIR/data/processed/

# Check if data copy succeeded
if [ ! -f "$TMPDIR/data/processed/tabular_dataset.joblib" ]; then
  echo "ERROR: Failed to copy tabular dataset file"
  exit 1
fi

# Set directories
export DATA_DIR=$TMPDIR/data

echo "Starting LightGBM training with data from $DATA_DIR"
echo "Results will be saved to $OUTPUT_DIR"
echo "Using $SLURM_CPUS_PER_TASK CPU cores"

# Run LightGBM module
srun python -m src.models.Tabular.LightGBM \
  --data_dir $DATA_DIR \
  --output_dir $OUTPUT_DIR

echo "Cleaning up scratch space..."
rm -rf $TMPDIR/data

echo "Job completed successfully!"