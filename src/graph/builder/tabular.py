import os 
import pandas as pd
import numpy as np
from typing import Optional, List
import joblib

# Logging setup
import logging, sys
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

class TabularBuilderMixin:

    ############################################################
    ################ Integrating existing data #################
    ############################################################

    ##############################
    # Integration: Weather
    ##############################

    def integrate_weather_features(self) -> None:
        """
        Merge weather features into self.tabular_feature_df on bucket_idx.
        Requires self.tabular_feature_df, self.weather_data_dict, and self.time_buckets to exist.
        """
        if not hasattr(self, 'time_buckets'):
            raise ValueError("time_buckets not found; ensure initialize_time_parameters() was called.")
        if not hasattr(self, 'weather_data_dict'):
            raise ValueError("weather_data_dict not found; call get_weather_data() first.")
        if not hasattr(self, 'tabular_feature_df'):
            raise ValueError("tabular_feature_df not found. Get it first.")

        # Convert weather dict to DataFrame
        weather_df = pd.DataFrame.from_dict(self.weather_data_dict, orient='index')
        weather_df = weather_df.reset_index().rename(columns={'index': 'bucket_idx'})

        # Merge on bucket_idx
        merged = pd.merge(
            self.tabular_feature_df,
            weather_df,
            on='bucket_idx',
            how='left'
        )

        # Store
        self.tabular_feature_df = merged
        return None
    
    ##############################
    # Integration: Time features
    ##############################

    def add_time_features(self) -> None:
        """
        Extract cyclical time features (hour, day of week) from the time buckets.
        Adds 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos' to tabular_feature_df.
        """
        if not hasattr(self, 'time_buckets'):
            raise ValueError("time_buckets not found; ensure initialize_time_parameters() was called.")
        if not hasattr(self, 'tabular_feature_df'):
            raise ValueError("tabular_feature_df not found. Get it first.")

        # Map bucket_idx to start timestamp
        ts_map = {i: tb[0] for i, tb in enumerate(self.time_buckets)}
        df = getattr(self, 'tabular_feature_df')
        df['timestamp'] = df['bucket_idx'].map(ts_map)
        dt = pd.to_datetime(df['timestamp'])

        # Hour of day
        hours = dt.dt.hour
        df['hour_sin'] = np.sin(2 * np.pi * hours / 24)
        df['hour_cos'] = np.cos(2 * np.pi * hours / 24)
        # Day of week
        dows = dt.dt.dayofweek
        df['dow_sin'] = np.sin(2 * np.pi * dows / 7)
        df['dow_cos'] = np.cos(2 * np.pi * dows / 7)
        
        # Drop helper timestamp
        df.drop(columns=['timestamp'], inplace=True)
        return None
    
    ############################################################
    ############# Block-aware feature engineering ##############
    ############################################################

    def _assign_block_id(self, df: pd.DataFrame) -> pd.Series:
        """
        Internal helper: map each bucket_idx to its block_id so we can group by block.
        """
        if not hasattr(self, 'blocks'):
            raise ValueError("self.blocks not found, ensure build_weekly_blocks() was called.")
        
        if not hasattr(self, "_block_map"):
            self._block_map = {
                idx: blk
                for blk, info in self.blocks.items()
                for idx in info["bucket_indices"]
            }
        return df["bucket_idx"].map(self._block_map)

    def add_moving_average_features(self,
                                    windows: List[int],
                                    shift_amount: int,
                                    data_frame: pd.DataFrame,
                                    cols: Optional[List[str]] = None,
                                    use_only_original_columns: bool = True,
                                    extra_grouping_cols: Optional[List[str]] = None
                                    ) -> pd.DataFrame:
        """
        For each col in `cols` (default: all numeric except 'bucket_idx'),
        create backward-looking moving averages of the past values,
        per block (so no cross‐block leakage).
                
        Args:
            windows: list of window sizes, e.g. [3, 24].
            shift_amount: how many steps to shift before averaging.
            data_frame: pd.DataFrame.
            cols: which columns to average; defaults to all numeric features.
            use_only_original_columns: whether to ignore columns generated by previous calls to 
                                       `add_moving_average_features` or `add_lag_features`).
                                       This prevents creating MAs of MAs or MAs of lags.
            extra_grouping_cols: any additional grouping columns (like 'room_number'), additional to ['block_id'].
        """
        df = data_frame.copy()

        # Assign block ID to df
        df["block_id"] = self._assign_block_id(df)

        # Grouping
        grouping = ['block_id']
        if extra_grouping_cols:
            grouping.extend(extra_grouping_cols)
        logger.info(f"Adding moving average features, grouping by: {grouping}")

        # Sorting based on grouping + bucket_idx
        df.sort_values(grouping + ['bucket_idx'], inplace=True)

        # Pick which columns to do moving average
        if cols is None:
            # start from every numeric column except bucket_idx/block_id
            all_num = df.select_dtypes("number").columns.tolist()
            candidates = [c for c in all_num if c not in ("bucket_idx", "block_id")]
            if use_only_original_columns:
                # filter out any generated MA or lag columns
                cols = [
                    c for c in candidates
                    if "_ma_" not in c and "_lag_" not in c
                ]
            else:
                cols = candidates

        # compute everything into a dict of Series
        moving_average_dict = {}
        gb = df.groupby(grouping, group_keys=False)
        for w in windows:
            for col in cols:
                series = gb[col].apply(
                    lambda g: g.shift(shift_amount).rolling(window=w, min_periods=1).mean()
                )
                moving_average_dict[f"{col}_ma_{w}_sh{shift_amount}"] = series

        # Concatenate all columns in one go
        ma_df = pd.DataFrame(moving_average_dict, index=df.index)
        df = pd.concat([df, ma_df], axis=1)

        # Drop helper
        df.drop(columns='block_id', inplace=True)

        return df

    def add_lag_features(self,
                        lags: List[int],
                        data_frame: pd.DataFrame,
                        cols: Optional[List[str]] = None,
                        use_only_original_columns: bool = True,
                        extra_grouping_cols: Optional[List[str]] = None,
                        ) -> pd.DataFrame:
        """
        For each col in `cols` (default: all numeric except 'bucket_idx'), 
        create lag‐k features per block (no leakage across train/val/test).

        Args:
            lags: list of integers, e.g. [1, 24]
            data_frame: pd.DataFrame.
            cols: which columns to lag; defaults to all numeric features.
            use_only_original_columns: whether to ignore columns generated by previous calls to 
                                       `add_moving_average_features` or `add_lag_features`).
                                       This prevents creating MAs of MAs or MAs of lags.
            extra_grouping_cols: any additional grouping columns (like 'room_number'), additional to ['block_id'].
        """
        df = data_frame.copy()

        # Assign block ID to df
        df["block_id"] = self._assign_block_id(df)

        # Grouping
        grouping = ['block_id']
        if extra_grouping_cols:
            grouping.extend(extra_grouping_cols)
        logger.info(f"Adding lag features, grouping by: {grouping}")

        # Sorting based on grouping + bucket_idx
        df.sort_values(grouping + ['bucket_idx'], inplace=True)

        # Pick which columns to lag
        if cols is None:
            # start from every numeric column except bucket_idx/block_id
            all_num = df.select_dtypes("number").columns.tolist()
            candidates = [c for c in all_num if c not in ("bucket_idx", "block_id")]
            if use_only_original_columns:
                # filter out any generated MA or lag columns
                cols = [
                    c for c in candidates
                    if "_ma_" not in c and "_lag_" not in c
                ]
            else:
                cols = candidates

        # For each lag and each column
        lag_dict = {}
        gb = df.groupby(grouping)
        for k in lags:
            for col in cols:
                lag_series = gb[col].shift(k)
                lag_dict[f"{col}_lag_{k}"] = lag_series

        # Concatenate all columns in one go
        lag_df = pd.DataFrame(lag_dict, index=df.index)
        df = pd.concat([df, lag_df], axis=1)

        # Drop helper
        df.drop(columns='block_id', inplace=True)

        return df
    
    ############################################################
    ################### Preparing Targets ######################
    ############################################################

    def add_consumption_to_df(self, data_frame: pd.DataFrame) -> pd.DataFrame:
        """
        Merge consumption values as the target column into a given df on bucket_idx.
        Requires self.consumption_values and self.time_buckets to exist.
        """
        if not hasattr(self, 'time_buckets'):
            raise ValueError("time_buckets not found; ensure initialize_time_parameters() was called.")
        if not hasattr(self, 'consumption_values'):
            raise ValueError("consumption_values not found; call get_consumption_values() first.")

        df = data_frame.copy()

        # Build consumption DataFrame
        n = len(self.time_buckets)
        cons_df = pd.DataFrame({
            'bucket_idx': list(range(n)),
            'consumption': self.consumption_values
        })
        merged_df = pd.merge(
            df,
            cons_df,
            on='bucket_idx',
            how='left'
        )

        return merged_df

    def add_target_consumption_to_df(self, data_frame: pd.DataFrame, horizon: int = 1) -> pd.DataFrame:
        """
        Creates the future consumption target by shifting the consumption values
        backwards within each block to prevent data leakage.

        This method adds a new 'target_consumption' column to the given data_frame.
        The data_frame must have a column named "consumption".

        Note that the rows where the target could not be created (i.e., the last `horizon` rows
        of each block) will have NaN and must be dropped before training.

        Args:
            horizon (int): The number of time steps into the future to predict.
                        Defaults to 1 (predict the next time step).
        """        
        if "consumption" not in data_frame.columns:
            raise KeyError("Required column 'consumption' not found in DataFrame.")

        logger.info(f"Creating block-aware consumption target for horizon={horizon}...")
        
        df = data_frame.copy()
        
        # 1. Assign block ID for grouping
        df['block_id'] = self._assign_block_id(df)
        
        # 2. Group by block and shift to get the future value
        #    .shift(-horizon) pulls future values into the current row.
        df['target_consumption'] = df.groupby('block_id')['consumption'].shift(-horizon)
        
        # 3. Drop the helper column
        df.drop(columns='block_id', inplace=True)
        
        # 4. Report on the number of NaNs created
        nan_count = df['target_consumption'].isna().sum()
        logger.info(f"Created 'target_consumption'. Found {nan_count} rows with NaN targets (these should be dropped).")
        
        return df
    
    def add_target_measurement_to_df(self, data_frame: pd.DataFrame, horizon: int = 1) -> pd.DataFrame:
        """
        Creates the future measurement target for a specific variable (e.g., Temperature).
        
        This is done by shifting the measurement values backwards within each block AND for
        each room to prevent data leakage. It adds a 'target_measurement' column.

        Args:
            stat (str): The statistic of the measurement variable to use as the target (e.g., 'mean').
            horizon (int): The number of time steps into the future to predict. Defaults to 1.
        """
        if "room_number" not in data_frame.columns:
            raise KeyError("This method is for measurement forecasting and requires 'room_number' in the DataFrame.")
        
        # The source column from which we create the target
        variable = self.measurement_variable
        stat = self.measurement_variable_stat
        source_column = f"{variable}_{stat}"
        if source_column not in data_frame.columns:
            raise ValueError(f"Source column '{source_column}' not found in the DataFrame.")
        
        logger.info(f"Creating block-aware measurement target from '{source_column}' for horizon={horizon}...")
        
        df = data_frame.copy()
        
        # 1. Assign block ID for grouping
        df['block_id'] = self._assign_block_id(df)
        
        # 2. IMPORTANT: Group by both block and room, then shift
        df['target_measurement'] = df.groupby(['block_id', 'room_number'])[source_column].shift(-horizon)
        
        # 3. Drop the helper column
        df.drop(columns='block_id', inplace=True)
        
        # 4. Report on the number of NaNs
        nan_count = df['target_measurement'].isna().sum()
        logger.info(f"Created 'target_measurement'. Found {nan_count} total rows with NaN targets.")
        
        return df
    
    ############################################################
    #################### Master Function #######################
    ############################################################
    
    def build_tabular_df(self, 
                         forecast_horizon: int = 1,
                         lags: List[int] = None,
                         windows: List[int] = None,
                         shift_amount: int = 1, 
                         integrate_weather: bool = True):
        """
        This is the final, all-in-one function to build tabular data depending on the 'build_mode' (or 'task_type').
        1) Handles building the base DataFrame, appropriate to the task type
        2) Does feature engineering.
        3) Adds the appropriate target column to the DataFrame, and cleans the rows where target is NaN.
        """

        ##### Building the base DataFrame #####

        if self.build_mode == "workhour_classification":
            # We have only a single floor (floor 7) for this task
            # So the floor_level_df is the same as building_level_df essentially
            self.build_floor_level_df()
            self.tabular_feature_df = self.floor_level_df.copy()
        
        elif self.build_mode == "consumption_forecast":
            # For this task, we have a single value per time bucket for the consumption of the whole building. 
            # Using all devices (160~), and adding MA and lag, etc. would be too much features
            # We can use either the floor-level or building-level DataFrames.
            # floor-level DataFrame is still a lot of features. So, for now, using building-level.
            self.build_floor_level_df()
            self.build_building_level_df()
            self.tabular_feature_df = self.building_level_df.copy()
        
        else: # self.build_mode == "measurement_forecast":
            logger.info("Using room_level_df when building tabular_feature_df for the 'measurement_forecast' task.")
            logger.info("Please check if you have added static room attributes before this step.")
            
            self.tabular_feature_df = self.room_level_df.copy()
            self.tabular_feature_df['room_number'] = (
                self.tabular_feature_df['room_URIRef'].map(self.office_graph._map_RoomURIRef_to_RoomNumber))
            self.tabular_feature_df['room_number'] = (
                self.tabular_feature_df['room_number'].astype('category'))
            self.tabular_feature_df.drop(columns=['room_URIRef'], inplace=True)
        
        # Ensuring features are appropriately integer/categorical
        # Has_measurement is binary:
        has_measurement_cols = [c for c in self.tabular_feature_df.columns if "has_measurement" in c]
        if has_measurement_cols:
            self.tabular_feature_df[has_measurement_cols] = (
                self.tabular_feature_df[has_measurement_cols].astype("category"))
        
        ##### Feature engineering #####

        # Adding weather features here so we get also their MAs and lags
        # NOTE: Not integrating it for workhour_classification,
        #       since the model would just ignore the IoT data
        if self.build_mode != "workhour_classification" and integrate_weather:
            self.integrate_weather_features()

        # Defining selective feature lists for lags and moving averages
        base_cols = self.tabular_feature_df.select_dtypes("number").columns.tolist()
        base_cols = [c for c in base_cols if c not in ('bucket_idx', 'block_id')]

        # Tier 1: Core signals for both Lags and MAs
        # All means, maxes, mins, and key weather variables
        core_signals_for_lags_and_ma = [
            c for c in base_cols if 
            any(k in c for k in ['_mean', '_max', '_min']) or
            c in ['temperature_2m', 'relative_humidity_2m', 'precipitation', 
                  'wind_speed_10m', 'wind_speed_80m', 'cloud_cover']
        ]
        logger.info(f"Generating lags and MAs for {len(core_signals_for_lags_and_ma)} core signal columns.")

        # Tier 2: Secondary signals for MA only
        secondary_signals_for_ma_only = [
            c for c in base_cols if 
            any(k in c for k in ['_std', '_count', '_n_devices', '_has_measurement'])
        ]
        logger.info(f"Generating MAs only for {len(secondary_signals_for_ma_only)} secondary signal columns.")

        if self.build_mode == "measurement_forecast":
            # For this task, we also have "room_number" as an additional grouping col
            extra_grouping_col = ["room_number"]
        else:
            extra_grouping_col = None

        # Defaults for MAs & Lags
        if lags is None:
            lags=[1, 2, 3]
        if windows is None:
            windows=[3, 6, 12, 24]

        # Creating MAs & Lags
        self.tabular_feature_df = self.add_lag_features(
            lags=lags,
            data_frame=self.tabular_feature_df,
            cols=core_signals_for_lags_and_ma,
            use_only_original_columns=True,
            extra_grouping_cols=extra_grouping_col)
        
        self.tabular_feature_df = self.add_moving_average_features(
            windows=windows,
            shift_amount=shift_amount,
            data_frame=self.tabular_feature_df,
            cols=core_signals_for_lags_and_ma + secondary_signals_for_ma_only,
            use_only_original_columns=True,
            extra_grouping_cols=extra_grouping_col)

        # NOTE 1: We can add the time features after taking MA & lag,
        #         as we should not really take the lag of the time-related features

        # NOTE 2: We should exclude time-related columns for the workhour_classification
        #         as time features are direct leakage for the classifying classifying
        #         whether an hour is work hour or not.
        if self.build_mode != "workhour_classification":
            self.add_time_features()
        
        ##### Target preparation #####
        # 1. Create the appropriate target column based on the task
        if self.build_mode == "workhour_classification":
            if not hasattr(self, 'workhour_labels'):
                raise ValueError("workhour_labels attribute not found for classification task.")
            self.tabular_df = self.tabular_feature_df.copy()
            self.target_col_name = 'workhour_labels'
            self.tabular_df[self.target_col_name] = self.workhour_labels
        
        elif self.build_mode == "consumption_forecast":
            self.tabular_df = self.add_consumption_to_df(
                data_frame=self.tabular_feature_df)
            
            # Creating MAs & Lags for consumption (feature engineering)
            self.tabular_df = self.add_lag_features(
                lags=lags,
                data_frame=self.tabular_df,
                cols=["consumption"],
                use_only_original_columns=True,
                extra_grouping_cols=extra_grouping_col)
            
            self.tabular_df = self.add_moving_average_features(
                windows=windows,
                shift_amount=shift_amount,
                data_frame=self.tabular_df,
                cols=["consumption"],
                use_only_original_columns=True,
                extra_grouping_cols=extra_grouping_col)
              
            # Adding the target
            self.target_col_name = 'target_consumption'
            self.tabular_df = self.add_target_consumption_to_df(
                data_frame=self.tabular_df,
                horizon=forecast_horizon)

        elif self.build_mode == "measurement_forecast":
            self.target_col_name = 'target_measurement'
            self.tabular_df = self.add_target_measurement_to_df(
                data_frame=self.tabular_feature_df,
                horizon=forecast_horizon)
        
        else:
            raise ValueError(f"Unknown build_mode: {self.build_mode}")

        # 2. CRITICAL: Drop all rows where the target is NaN.
        # For both consumption_forecast & measurement_forecast, we have:
        # - end-of-block NaNs
        # For measurement_forecast, we have:
        # - missing measurements in certain time buckets, so NaNs in target cols.
        initial_rows = len(self.tabular_df)
        self.tabular_df.dropna(subset=[self.target_col_name], inplace=True)
        final_rows = len(self.tabular_df)
        logger.info(f"Dropped {initial_rows - final_rows} rows with NaN targets. {final_rows} rows remaining.")

        if self.tabular_df.empty:
            raise ValueError("No valid data rows remaining after dropping NaN targets.")

        return None

    def save_tabular_df(self, output_path: str) -> None:
        """
        Saves tabular_df.
        """
        if not hasattr(self, 'tabular_df'):
            raise ValueError("tabular_df not found. Run build_tabular_df() first.")
        
        logger.info(f"Saving tabular data for task: {self.build_mode}")

        # 3. Create the final payload to save
        tabular_input = {
            "df": self.tabular_df,
            "target_col_name": self.target_col_name,
            "blocks": self.blocks,
            "task_type": self.build_mode
        }
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Save the dictionary using joblib
        joblib.dump(tabular_input, output_path)
        logger.info(f"Successfully saved final DataFrame to {output_path}")
        return None